{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weizhuo-Zhang/CS188_AI/blob/master/cs188_notes/4_Markov_Decision_Process_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03pSXau-gFxo",
        "colab_type": "text"
      },
      "source": [
        "# Markov Decision Processes\n",
        "\n",
        "## MDP Definition\n",
        "\n",
        "- A **set of states $s \\in S$**\n",
        "- A **set of actions $a \\in A$**\n",
        "- A **transition function $T(s,a,s')$**\n",
        "  - Probability that a from s leads to s’, i.e., $P(s' | s, a)$\n",
        "  - Also called the model or the dynamics\n",
        "- A **reward function $R(s,a,s')$**\n",
        "  - Sometimes just $R(s)$ or $R(s')$\n",
        "- A **start state**\n",
        "- Maybe a **terminal state**\n",
        "\n",
        "## MDPs are non-deterministic search problems\n",
        "- One way to solve them is with **expectimax search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qQshljNsbRj",
        "colab_type": "text"
      },
      "source": [
        "## What is Markov about MDPs\n",
        "- \"Markov\" generally means that given the present state, the future and the past are independent\n",
        "- For Markov Decision Processes, \"Markov\" means action outcomes depend only on the current state (not the history)\n",
        "$$P(S_{t+1}=s'|S_t=s_t, A_t=a_t, S_{t-1}, A_{t-1},...S_0=s_0)$$\n",
        "$$=$$\n",
        "$$P(S_{t+1}=s'|S_t=s_t, A_t=a_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhAb-pJFsdv3",
        "colab_type": "text"
      },
      "source": [
        "## Policies\n",
        "- In deterministic single-agent search problems,\n",
        "we wanted an optimal **plan**, or sequence of\n",
        "actions, from start to a goal\n",
        "- For MDPs, we want an optimal **policy $\\pi^*: S \\to A$**\n",
        "  - A policy $\\pi$ gives an action for each state\n",
        "  - An optimal policy is one that maximizes\n",
        "expected utility if followed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6-ZjaOJsgCt",
        "colab_type": "text"
      },
      "source": [
        "## Discounting $\\gamma$\n",
        "\n",
        "- It’s reasonable to **maximize the sum of rewards**\n",
        "- It’s also reasonable to **prefer rewards now** to rewards later\n",
        "\n",
        "### How to discount\n",
        "\n",
        "- Each time we descend a level, we\n",
        "multiply in the discount $\\gamma$ once\n",
        "![discounting](https://raw.githubusercontent.com/Weizhuo-Zhang/CS188_AI/master/cs188_notes/pics/discounting.png?token=AHRFC2L22XUZ7G3CQNZBSNS5LOLCE)\n",
        "\n",
        "### Why is discount\n",
        "- Sooner rewards probably do have higher utility than later rewards\n",
        "- Also helps our algorithms converge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mheRijdaygYk",
        "colab_type": "text"
      },
      "source": [
        "## Optimal Quantities\n",
        "\n",
        "- **The value (utility) of a state $s$**\n",
        "  - $V^*(s)=$ expected utility starting in $s$ and acting optimally\n",
        "\n",
        "- **The value (utility) of a q-state $(s,a)$**\n",
        "  - $Q^*(s,a)=$ expected utility starting out having taken action $a$ from state $s$ and (thereafter) acting optimally\n",
        "\n",
        "- **The optimal policy**\n",
        "  - $\\pi^*(s)=$ optimal action from state $s$\n",
        "\n",
        "![optimal_quantities](https://raw.githubusercontent.com/Weizhuo-Zhang/CS188_AI/master/cs188_notes/pics/optimal_quantities.png?token=AHRFC2MBYGICLZJL2KH4OE25LOOC6)\n",
        "\n",
        "![values and q values](https://raw.githubusercontent.com/Weizhuo-Zhang/CS188_AI/master/cs188_notes/pics/value_and_q_value.png?token=AHRFC2NGDTH5MJBCKL4JUWK5LOQIW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyCuH-FP4xvx",
        "colab_type": "text"
      },
      "source": [
        "## Time-Limited Values\n",
        "\n",
        "- Key idea: time-limited values\n",
        "- Define $V_k(s)$ to be the optimal value of s if the game ends in $k$ more time steps\n",
        "\n",
        "## Computing Time-Limited Values\n",
        "\n",
        "![computing time-limited values](https://raw.githubusercontent.com/Weizhuo-Zhang/CS188_AI/master/cs188_notes/pics/computing_time-limited_values.png?token=AHRFC2OFEGK4OZQY7NIS5QS5LOSVI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFsBkiX8rmo",
        "colab_type": "text"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8gAQ2ubsg9n",
        "colab_type": "text"
      },
      "source": [
        "## Bellman Equations\n",
        "\n",
        "Definition of “optimal utility” via expectimax recurrence gives a simple one-step lookahead relationship amongst optimal utility values。\n",
        "\n",
        "$$V^*(s)=\\max_a Q^*(s,a)$$\n",
        "\n",
        "$$Q^*(s,a)=\\sum_{s'}T(s,a,s')\\left[ R(s,a,s') + \\gamma V^*(s') \\right]$$\n",
        "\n",
        "$$V^*(s) = \\max_a \\sum_{s'}T(s,a,s') \\left[ R(s,a,s') + \\gamma V^*(s') \\right]$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXXHrIyKhakl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}