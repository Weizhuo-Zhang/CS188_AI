{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_Bayes_Net_Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weizhuo-Zhang/CS188_AI/blob/master/cs188_notes/6_Bayes_Net_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb34xSejm9IO",
        "colab_type": "text"
      },
      "source": [
        "#Bayes Net\n",
        "\n",
        "**Bayes's nets**: a technique for describing complex joint distributions (models) using simple, local distributions (conditional probabilities)\n",
        "- A subset of the general class of **graphical models**\n",
        "\n",
        "## Probability Inference (概率推理)\n",
        "\n",
        "### Inference\n",
        "- calculating some useful quantity from a joint probability distribution\n",
        "- Examples:\n",
        "  - **Posterior Probability**: a bunch of evidence variables $e_i$ gthat we already know, and we care about the probability of $Q$. And there are also a bunch of other variables called **hidden variables** that we don't care sbout and we don't observe.\n",
        "$$P(Q|E_1=e_1,...E_k=e_k)$$\n",
        "\n",
        "### Inference by Enumeration\n",
        "\n",
        "- General case:\n",
        "  - Evidence variables: $E_1...E_k=e_1...e_k$\n",
        "  - Query* variable: $Q$\n",
        "  - Hidden variables: $H_1...H_r$\n",
        "\n",
        "![inference_by_enumeration](https://github.com/Weizhuo-Zhang/CS188_AI/blob/master/cs188_notes/pics/inference_by_enumeration.png?raw=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s-kNxCedheB",
        "colab_type": "text"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "- Sampling is a lot like **repeated simulation**\n",
        "  - Predicting the weather, basketball games, ...\n",
        "\n",
        "- **Basic idea**\n",
        "  - Draw N samples from a sampling distribution S\n",
        "  - Compute an approximate posterior probability\n",
        "  - Show this converges to the true probability P\n",
        "\n",
        "- **Why sample?**\n",
        "  - Learning: get samples from a distibution you don't know\n",
        "  - Inference: getting a sample is faster than computing the right answer (e.g. with variable elimination)\n",
        "\n",
        "### Sampling in Bayes' Nets\n",
        "- **Prior Sampling**\n",
        "- **Rejection Sampling**\n",
        "- **Likelihood Weighting**\n",
        "- **Gibbs Sampling**\n"
      ]
    }
  ]
}