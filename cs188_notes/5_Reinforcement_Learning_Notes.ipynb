{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reinforcement Learning\n",
    "\n",
    "## Active Reinforcement Learning\n",
    "\n",
    "Active Reinforcement Learning collects data while learning from Q-Values.\n",
    "\n",
    "Full reinforcement learning: optimal policies (like value iteration)\n",
    "- You don't know the transitions T(s,a,s')\n",
    "- You don't know the rewards R(s,a,s')\n",
    "- You choose the actions now\n",
    "- <font color=\"red\">Goal: learn the optimal policy / values</font>\n",
    "\n",
    "In this case:\n",
    "- Learner makes choices!\n",
    "- Fundamental tradeoff: exploration vs. exploitation\n",
    "- This is NOT offline planning! You actually take actions in the world and find out what happens...\n",
    "\n",
    "\n",
    "## Detour: Q-Value Interation\n",
    "Value iteration: find successive (depth-limited) values\n",
    "- Start with V<sub>0</sub>(s)=0, which we know is right\n",
    "- Given V<sub>k</sub>, calculate the depth k+1 values for all states:\n",
    "\n",
    "　　　　$$V_{k+1}(s)\\leftarrow\\mathop{\\max}_a\\sum_{s'} T(s,a,s')[R(s,a,s')+\\gamma V_k(s')]$$\n",
    "    \n",
    "But Q-Values are more useful, so compute them instead\n",
    "- Start with Q<sub>0</sub>(s,a)=0, which we know is right\n",
    "- Given Q<sub>k</sub>, calculate the depth k+1 q-values for all q-states:\n",
    "\n",
    "　　　　$$Q_{k+1}(s,a)\\leftarrow\\sum_{s'} T(s,a,s')[R(s,a,s')+\\gamma \\mathop{\\max}_{a'}Q_k(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs. Exploitation\n",
    "\n",
    "- [**Exploration**](#Exploration): you have to try unknown actions to get information\n",
    "- **Exploitation**: eventually, you have to use what you know\n",
    "\n",
    "## Exploration\n",
    "\n",
    "### $\\varepsilon-greedy$ Random actions (simplest)\n",
    "- **Steps:**\n",
    "  - Every time step, flip a coin\n",
    "  - With (small) probability $\\varepsilon$, act randomly\n",
    "  - With (large) probability $1-\\varepsilon$, act on current policy\n",
    "\n",
    "- **Problems with random actions**\n",
    "  - You do eventually explore the space, but keep thrashing around once learning is done\n",
    "  - One solution: lower $\\varepsilon$ over time\n",
    "  - Another solution: [exploration functions](#Exploration-Functions)\n",
    "\n",
    "### Exploration Functions\n",
    "- Random actions: explore a fixed amount\n",
    "- Better idea: explore areas whose badness is not (yet) established, eventually stop exploring\n",
    "- Take a value estimate $u$ and a visit count $n$, and returns an optimistic utility, e.g, $f(u,n)=u+k/n$\n",
    "  - Regular Q-Update: $Q(s,a)\\leftarrow_\\alpha R(s,a,s')+\\gamma\\mathop{\\max}_{\\alpha'}Q(s',a')$\n",
    "  - Modified Q-Update: $Q(s,a)\\leftarrow_\\alpha R(s,a,s')+\\gamma\\mathop{\\max}_{\\alpha'}f(Q(s',a'),N(s',a'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret\n",
    "- **Measure how quick your learner get the optimal solution, quicker indicates less regret which is better.**\n",
    "- Even if you learn the optimal policy, you still make mistakes alone the way!\n",
    "- Regret is a measure of your total mistake cost: the difference between your (expected) rewards, including youthful suboptimality, and optimal(expexted) rewards\n",
    "- Minimizing regret goes beyond learning to be optimal - it requires optimally learning to be optimal\n",
    "- Example: random exploration and exploration functions both end up optimal, but random exploration has higher regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A\n",
    "- What is the complexity of the Q-Learning?\n",
    "- Does Q-Learning have to explor as many as MDP? (online, offline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
