{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "5_Reinforcement_Learning_Notes.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iey13Bfif3R",
        "colab_type": "text"
      },
      "source": [
        "# 5. Reinforcement Learning\n",
        "\n",
        "![model based and model free](https://raw.githubusercontent.com/Weizhuo-Zhang/CS188_AI/master/cs188_notes/pics/model_based_model_free.png?token=AHRFC2JG6OIGIPI6DKNKIJC5MDW6E)\n",
        "\n",
        "## Model-Based Learning\n",
        "### Model-Based Idea:\n",
        "- Learn an approximate model based on experiences\n",
        "- Solve for values as if the learned model were correct\n",
        "\n",
        "### Step 1: Learn empirical MDP model\n",
        "- Count outcomes $s'$ for each $s,a$\n",
        "- Normalize to give an estimate of $\\hat{T}(s,a,s')$\n",
        "- Discover each $\\hat{R}(s,a,s')$ when we experience $(s,a,s')$\n",
        "\n",
        "### Step 2: Solve the learned MDP\n",
        "- For example, use value iteration, as before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he2kg2GdldcT",
        "colab_type": "text"
      },
      "source": [
        "## Passive Reinforcement Learning\n",
        "### Simplified Task: policy evaluation\n",
        "- Input: a fixed policy $\\pi(s)$\n",
        "- You don't know the transitions $T(s,a,s')$\n",
        "- You don't know the rewards $R(s,a,s')$\n",
        "- **Goal: learn the state values**\n",
        "\n",
        "### In this case\n",
        "- Learner is \"along for the ride\"\n",
        "- No choice about what actions to take\n",
        "- Just execute the policy and learn from experience\n",
        "- This is NOT offline planning! You actually take actions in the world\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVnGEwIOmwcR",
        "colab_type": "text"
      },
      "source": [
        "## Temporal Difference Learning (即时差分学习)\n",
        "### Big idea: Learn from every experience\n",
        "- Update $V(s)$ each time we experience a transition $(s,a,s',r)$\n",
        "- Likely outcomes $s'$ will contribute updates more often\n",
        "\n",
        "### Temporal Difference Learning\n",
        "- Policy still fixed, still doing evaluation!\n",
        "- Move values toward value of whatever successor occurs: running average\n",
        "\n",
        "**Sample of V(s):** $$sample = R(s,\\pi(s),s')+\\gamma V^\\pi(s')$$\n",
        "\n",
        "**Update to V(s):**$$V^\\pi \\gets (1-\\alpha)V^\\pi(s)+(\\alpha)sample$$\n",
        "\n",
        "**Same update:**$$V^\\pi(s)\\gets V^\\pi(s)+\\alpha(sample-V^\\pi(s))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay4PXYJGtFO4",
        "colab_type": "text"
      },
      "source": [
        "## Exponential Moving Average (指数移动平均法)\n",
        "- The running interpolation update: $\\overline{x}_n=(1-\\alpha)\\cdot\\overline{x}_{n-1}+\\alpha\\cdot x_n$\n",
        "- Makes recent samples more important\n",
        "$$\\overline{x}_n=\\frac{x_n+(1-\\alpha)\\cdot x_{n-1}+(1-\\alpha)^2\\cdot x_{n-2}+...}{1+(1-\\alpha)+(1-\\alpha)^2+...}$$\n",
        "- Forgets about the past (distant past values were wrong anyway)\n",
        "- **Decreasing learning rate ($\\alpha$) can give converging averages**\n",
        "\n",
        "$$V^\\pi \\gets (1-\\alpha)V^\\pi(s)+\\alpha\\left[R(s,\\pi(s),s')+\\gamma V^\\pi(s')\\right]$$\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhIN6_Zviyfi",
        "colab_type": "text"
      },
      "source": [
        "## Active Reinforcement Learning\n",
        "\n",
        "Active Reinforcement Learning collects data while learning from Q-Values.\n",
        "\n",
        "Full reinforcement learning: optimal policies (like value iteration)\n",
        "- You don't know the transitions T(s,a,s')\n",
        "- You don't know the rewards R(s,a,s')\n",
        "- You choose the actions now\n",
        "- <font color=\"red\">Goal: learn the optimal policy / values</font>\n",
        "\n",
        "In this case:\n",
        "- Learner makes choices!\n",
        "- Fundamental tradeoff: exploration vs. exploitation\n",
        "- This is NOT offline planning! You actually take actions in the world and find out what happens...\n",
        "\n",
        "\n",
        "## Detour: Q-Value Interation\n",
        "Value iteration: find successive (depth-limited) values\n",
        "- Start with V<sub>0</sub>(s)=0, which we know is right\n",
        "- Given V<sub>k</sub>, calculate the depth k+1 values for all states:\n",
        "\n",
        "　　　　$$V_{k+1}(s)\\leftarrow\\mathop{\\max}_a\\sum_{s'} T(s,a,s')[R(s,a,s')+\\gamma V_k(s')]$$\n",
        "    \n",
        "But Q-Values are more useful, so compute them instead\n",
        "- Start with Q<sub>0</sub>(s,a)=0, which we know is right\n",
        "- Given Q<sub>k</sub>, calculate the depth k+1 q-values for all q-states:\n",
        "\n",
        "　　　　$$Q_{k+1}(s,a)\\leftarrow\\sum_{s'} T(s,a,s')[R(s,a,s')+\\gamma \\mathop{\\max}_{a'}Q_k(s',a')]$$\n",
        "    \n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiycMwY1if3U",
        "colab_type": "text"
      },
      "source": [
        "## Exploration vs. Exploitation\n",
        "\n",
        "- [**Exploration**](#Exploration): you have to try unknown actions to get information\n",
        "- **Exploitation**: eventually, you have to use what you know\n",
        "\n",
        "## Exploration\n",
        "\n",
        "### $\\varepsilon-greedy$ Random actions (simplest)\n",
        "- **Steps:**\n",
        "  - Every time step, flip a coin\n",
        "  - With (small) probability $\\varepsilon$, act randomly\n",
        "  - With (large) probability $1-\\varepsilon$, act on current policy\n",
        "\n",
        "- **Problems with random actions**\n",
        "  - You do eventually explore the space, but keep thrashing around once learning is done\n",
        "  - One solution: lower $\\varepsilon$ over time\n",
        "  - Another solution: [exploration functions](#Exploration-Functions)\n",
        "\n",
        "### Exploration Functions\n",
        "- Random actions: explore a fixed amount\n",
        "- Better idea: explore areas whose badness is not (yet) established, eventually stop exploring\n",
        "- Take a value estimate $u$ and a visit count $n$, and returns an optimistic utility, e.g, $f(u,n)=u+k/n$\n",
        "  - Regular Q-Update: $Q(s,a)\\leftarrow_\\alpha R(s,a,s')+\\gamma\\mathop{\\max}_{\\alpha'}Q(s',a')$\n",
        "  - Modified Q-Update: $Q(s,a)\\leftarrow_\\alpha R(s,a,s')+\\gamma\\mathop{\\max}_{\\alpha'}f(Q(s',a'),N(s',a'))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI4cUCR8if3V",
        "colab_type": "text"
      },
      "source": [
        "## Regret\n",
        "- **Measure how quick your learner get the optimal solution, quicker indicates less regret which is better.**\n",
        "- Even if you learn the optimal policy, you still make mistakes alone the way!\n",
        "- Regret is a measure of your total mistake cost: the difference between your (expected) rewards, including youthful suboptimality, and optimal(expexted) rewards\n",
        "- Minimizing regret goes beyond learning to be optimal - it requires optimally learning to be optimal\n",
        "- Example: random exploration and exploration functions both end up optimal, but random exploration has higher regret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLGkkFKMif3V",
        "colab_type": "text"
      },
      "source": [
        "# Q&A\n",
        "- What is the complexity of the Q-Learning?\n",
        "- Does Q-Learning have to explor as many as MDP? (online, offline)"
      ]
    }
  ]
}